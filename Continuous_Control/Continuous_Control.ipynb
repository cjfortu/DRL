{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.30 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mjupyter-console 6.4.3 has requirement jupyter-client>=7.0.0, but you'll have jupyter-client 5.2.4 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# # select this option to load version 2 (with 20 agents) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64', no_graphics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import workspace_utils.py\n",
    "from workspace_utils import active_session\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "import copy\n",
    "from itertools import repeat\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "# from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the default brain\n",
    "# brain_name = env.brain_names[0]\n",
    "# brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reset the environment\n",
    "# env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# # number of agents\n",
    "# num_agents = len(env_info.agents)\n",
    "# print('Number of agents:', num_agents)\n",
    "\n",
    "# # size of each action\n",
    "# action_size = brain.vector_action_space_size\n",
    "# print('Size of each action:', action_size)\n",
    "\n",
    "# # examine the state space \n",
    "# states = env_info.vector_observations\n",
    "# state_size = states.shape[1]\n",
    "# print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "# print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "# states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "# scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "# while True:\n",
    "#     actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#     actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#     env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#     print(env_info.rewards)\n",
    "#     next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#     rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#     dones = env_info.local_done                        # see if episode finished\n",
    "#     scores += env_info.rewards                         # update the score (for each agent)\n",
    "#     states = next_states                               # roll over states to next time step\n",
    "#     if np.any(dones):                                  # exit loop if episode finished\n",
    "#         break\n",
    "# print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Student Solution\n",
    "\n",
    "This is based on the Udacity DDPG examples:  \n",
    "https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum  \n",
    "https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-bipedal\n",
    "\n",
    "The changes mainly involved:\n",
    "* Modifying for the Unity API\n",
    "* Setting the stepping/learning scheme for multiple arms\n",
    "* Implementing batch normalization\n",
    "* Shrinking/simplifying the networks\n",
    "* Detaching networks not requiring gradient computation\n",
    "* Tuning hyperparameters based on the DDPG paper, the problem/environment, and observed behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First we create the network classes (Actor and Critic):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    # shrink/simplify the network from the DDPG example, for reduced computation\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        # Add batch normalization per the DDPG paper part 3\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)  \n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        if state.dim() == 1:\n",
    "            state = torch.unsqueeze(state,0)\n",
    "        # Add batch normalization per the DDPG paper part 3\n",
    "        x = F.relu(self.bn1(self.fc1(state)))  \n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    # shrink/simplify the network from the DDPG example, for reduced computation\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=128, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        # Add batch normalization per the DDPG paper part 3\n",
    "        self.bn1 = nn.BatchNorm1d(fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        if state.dim() == 1:\n",
    "            state = torch.unsqueeze(state,0)\n",
    "        # Add batch normalization per the DDPG paper part 3\n",
    "        xs = F.relu(self.bn1(self.fcs1(state)))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then we set hyperparameters and define the Agent class:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(20 * 1e5)  # larger buffer size to accomodate multiple arms\n",
    "BATCH_SIZE = 1024  # large batch size for more learning at the cost of more computation\n",
    "GAMMA = 0.99  # keep high, since we are going out to 1000 time steps\n",
    "TAU = 1e-3  # per DDPG paper recommendation part 7\n",
    "LR_ACTOR = 5e-4  # split the difference from LR_CRITIC in DDPG paper part 7\n",
    "LR_CRITIC = 5e-4  # split the difference from LR_ACTOR in DDPG paper part 7\n",
    "UPDATE_EVERY = 20  # per the lesson benchmark implementation\n",
    "UPDATE_QTY = 10  # per the lesson benchmark implementation\n",
    "WEIGHT_DECAY = 0  # L2 weight decay (from the Udacity DDPG examples)\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed, num_agents):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        self.num_agents = num_agents\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC,\\\n",
    "                                           weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise(action_size, random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)       \n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones, step):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        for i in range(self.num_agents):\n",
    "            self.memory.add(states[i], actions[i], rewards[i], next_states[i], dones[i])\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps and...\n",
    "        # ...if enough samples are available in memory.\n",
    "        if (step % UPDATE_EVERY == 0) and len(self.memory) > BATCH_SIZE:\n",
    "            for _ in range(UPDATE_QTY):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "            \n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        # Use noise to induce exploration, per the DDPG paper part 3\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models.\n",
    "        # The targets should be detached...\n",
    "        # ...since we are only computing gradients for the local networks.\n",
    "        actions_next = self.actor_target(next_states).detach()\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next).detach()\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        \n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        # per the lesson benchmark implementation\n",
    "        torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        # to maximize the loss function (required for gradient ascent)...\n",
    "        # ...we take the negative of the loss.\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now set the Noise class:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:  # Use noise to induce exploration, per the DDPG paper part 3\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.1):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the memory structure class for storing/sampling experiences (Replay Buffer):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\",\\\n",
    "                field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).\\\n",
    "                float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).\\\n",
    "                float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).\\\n",
    "                float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).\\\n",
    "                float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).\\\n",
    "                astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define the DDPG algorithm:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(n_episodes = 300, max_t=1000, print_every = 1, min_episodes = 100, min_score = 30):\n",
    "    scores_all_deque = deque(maxlen = min_episodes)  # the scores for computing performance\n",
    "    scores_all = []  # the scores for plotting the result\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]  # reset the environment\n",
    "        states = env_info.vector_observations  # get the current states (for each arm)\n",
    "        agent.reset()  # reset the agent for a new episode\n",
    "        scores = np.zeros(num_agents)  # initialize the scores for the episode\n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states)  # select actions based on the states\n",
    "\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next states (for each arm)\n",
    "            rewards = env_info.rewards                         # get rewards (for each arm)\n",
    "            dones = env_info.local_done                        # see if each arm episode finished\n",
    "\n",
    "            # Perform an agent step. The step function the lists for each s, a, r, ns, d...\n",
    "            # ... and passes the time step\n",
    "            agent.step(states, actions, rewards, next_states, dones, t)\n",
    "\n",
    "            states = next_states  # update the current states\n",
    "            scores += rewards  # log the scores for the episode, for each arm\n",
    "            if any(dones):  # break the episode if any arm finishes\n",
    "                print('end of episode {} for arm {}'.format(i_episode, np.where(np.array(dones) == True)[0]))\n",
    "                break \n",
    "        episode_mean = np.mean(scores)\n",
    "        scores_all_deque.append(episode_mean)\n",
    "        scores_all.append(episode_mean)\n",
    "        batch_mean = np.mean(scores_all_deque)  # compute performance for episode\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')  # save actor weights\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')  # save critic weights\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}  Episode Score {:.3f}  Cumulative Average Score: {:.3f}  time: {}'.\\\n",
    "                  format(i_episode, episode_mean, batch_mean, time.time() - start_time))\n",
    "        if batch_mean >= min_score and i_episode >= min_episodes:\n",
    "            print('\\nEnvironment solved in {:d} episodes!  Cumulative100 Average Score: {:.3f}'.\\\n",
    "                  format(i_episode, batch_mean))\n",
    "            break\n",
    "\n",
    "    return scores_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set the environment and device:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "# create the multi-arm environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64', no_graphics = True)\n",
    "\n",
    "# create the brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# get the basic environment info\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now within an active session (for non-idling workspaces):**\n",
    "* create the networks, agent, noise process, and replay buffer based on their classes\n",
    "* train\n",
    "* plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:103: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1  Episode Score 0.480  Cumulative Average Score: 0.480  time: 33.560805797576904\n",
      "Episode 2  Episode Score 0.711  Cumulative Average Score: 0.596  time: 58.11936855316162\n",
      "Episode 3  Episode Score 0.677  Cumulative Average Score: 0.623  time: 79.47827792167664\n",
      "Episode 4  Episode Score 0.809  Cumulative Average Score: 0.669  time: 101.20164465904236\n",
      "Episode 5  Episode Score 0.790  Cumulative Average Score: 0.694  time: 123.23350882530212\n",
      "Episode 6  Episode Score 0.927  Cumulative Average Score: 0.733  time: 145.59942865371704\n",
      "Episode 7  Episode Score 1.098  Cumulative Average Score: 0.785  time: 168.2777259349823\n",
      "Episode 8  Episode Score 1.301  Cumulative Average Score: 0.849  time: 191.42365908622742\n",
      "Episode 9  Episode Score 1.223  Cumulative Average Score: 0.891  time: 214.89563989639282\n",
      "Episode 10  Episode Score 1.330  Cumulative Average Score: 0.935  time: 239.06217002868652\n",
      "Episode 11  Episode Score 1.684  Cumulative Average Score: 1.003  time: 263.3274230957031\n",
      "Episode 12  Episode Score 1.875  Cumulative Average Score: 1.076  time: 288.1816620826721\n",
      "Episode 13  Episode Score 2.467  Cumulative Average Score: 1.183  time: 313.51476979255676\n",
      "Episode 14  Episode Score 2.683  Cumulative Average Score: 1.290  time: 339.602144241333\n",
      "Episode 15  Episode Score 2.475  Cumulative Average Score: 1.369  time: 366.5636365413666\n",
      "Episode 16  Episode Score 2.320  Cumulative Average Score: 1.428  time: 394.07823157310486\n",
      "Episode 17  Episode Score 2.966  Cumulative Average Score: 1.519  time: 422.235111951828\n",
      "Episode 18  Episode Score 3.315  Cumulative Average Score: 1.619  time: 451.3910901546478\n",
      "Episode 19  Episode Score 3.377  Cumulative Average Score: 1.711  time: 481.6302170753479\n",
      "Episode 20  Episode Score 3.568  Cumulative Average Score: 1.804  time: 512.4718174934387\n",
      "Episode 21  Episode Score 4.990  Cumulative Average Score: 1.956  time: 544.2378208637238\n",
      "Episode 22  Episode Score 4.482  Cumulative Average Score: 2.071  time: 576.6990518569946\n",
      "Episode 23  Episode Score 6.296  Cumulative Average Score: 2.254  time: 610.1763372421265\n",
      "Episode 24  Episode Score 6.893  Cumulative Average Score: 2.448  time: 644.7379040718079\n",
      "Episode 25  Episode Score 6.831  Cumulative Average Score: 2.623  time: 680.4388236999512\n",
      "Episode 26  Episode Score 8.575  Cumulative Average Score: 2.852  time: 717.1628437042236\n",
      "Episode 27  Episode Score 11.188  Cumulative Average Score: 3.161  time: 755.039763212204\n",
      "Episode 28  Episode Score 11.621  Cumulative Average Score: 3.463  time: 793.9699161052704\n",
      "Episode 29  Episode Score 13.171  Cumulative Average Score: 3.798  time: 833.8500998020172\n",
      "Episode 30  Episode Score 11.893  Cumulative Average Score: 4.067  time: 874.6235549449921\n",
      "Episode 31  Episode Score 15.195  Cumulative Average Score: 4.426  time: 916.4884669780731\n",
      "Episode 32  Episode Score 18.967  Cumulative Average Score: 4.881  time: 959.5966210365295\n",
      "Episode 33  Episode Score 24.462  Cumulative Average Score: 5.474  time: 1003.4464964866638\n",
      "Episode 34  Episode Score 22.675  Cumulative Average Score: 5.980  time: 1048.3245871067047\n",
      "Episode 35  Episode Score 24.587  Cumulative Average Score: 6.512  time: 1094.2732377052307\n",
      "Episode 36  Episode Score 23.020  Cumulative Average Score: 6.970  time: 1141.3934488296509\n",
      "Episode 37  Episode Score 22.943  Cumulative Average Score: 7.402  time: 1189.489783525467\n",
      "Episode 38  Episode Score 23.112  Cumulative Average Score: 7.815  time: 1238.6288287639618\n",
      "Episode 39  Episode Score 24.493  Cumulative Average Score: 8.243  time: 1288.5096213817596\n",
      "Episode 40  Episode Score 27.223  Cumulative Average Score: 8.718  time: 1339.3348104953766\n",
      "Episode 41  Episode Score 27.677  Cumulative Average Score: 9.180  time: 1391.262387752533\n",
      "Episode 42  Episode Score 27.118  Cumulative Average Score: 9.607  time: 1444.3819015026093\n",
      "Episode 43  Episode Score 30.504  Cumulative Average Score: 10.093  time: 1498.2285768985748\n",
      "Episode 44  Episode Score 29.656  Cumulative Average Score: 10.538  time: 1553.1324532032013\n",
      "Episode 45  Episode Score 31.480  Cumulative Average Score: 11.003  time: 1609.247406244278\n",
      "Episode 46  Episode Score 31.426  Cumulative Average Score: 11.447  time: 1666.1607756614685\n",
      "Episode 47  Episode Score 34.322  Cumulative Average Score: 11.934  time: 1724.2934458255768\n",
      "Episode 48  Episode Score 33.300  Cumulative Average Score: 12.379  time: 1783.3389945030212\n",
      "Episode 49  Episode Score 33.746  Cumulative Average Score: 12.815  time: 1843.1815121173859\n",
      "Episode 50  Episode Score 33.580  Cumulative Average Score: 13.230  time: 1904.07763338089\n",
      "Episode 51  Episode Score 34.222  Cumulative Average Score: 13.642  time: 1965.9935159683228\n",
      "Episode 52  Episode Score 34.847  Cumulative Average Score: 14.050  time: 2028.692619085312\n",
      "Episode 53  Episode Score 36.556  Cumulative Average Score: 14.474  time: 2092.2828147411346\n",
      "Episode 54  Episode Score 36.425  Cumulative Average Score: 14.881  time: 2157.471260070801\n",
      "Episode 55  Episode Score 36.719  Cumulative Average Score: 15.278  time: 2223.2067832946777\n",
      "Episode 56  Episode Score 37.107  Cumulative Average Score: 15.668  time: 2290.1873371601105\n",
      "Episode 57  Episode Score 35.367  Cumulative Average Score: 16.013  time: 2358.0149760246277\n",
      "Episode 58  Episode Score 35.808  Cumulative Average Score: 16.355  time: 2427.050705909729\n",
      "Episode 59  Episode Score 36.369  Cumulative Average Score: 16.694  time: 2497.019347190857\n",
      "Episode 60  Episode Score 37.280  Cumulative Average Score: 17.037  time: 2568.0670001506805\n",
      "Episode 61  Episode Score 37.657  Cumulative Average Score: 17.375  time: 2640.171636581421\n",
      "Episode 62  Episode Score 36.441  Cumulative Average Score: 17.682  time: 2713.0995333194733\n",
      "Episode 63  Episode Score 37.348  Cumulative Average Score: 17.995  time: 2787.0752091407776\n",
      "Episode 64  Episode Score 37.547  Cumulative Average Score: 18.300  time: 2862.2318975925446\n",
      "Episode 65  Episode Score 37.153  Cumulative Average Score: 18.590  time: 2938.2378482818604\n",
      "Episode 66  Episode Score 36.808  Cumulative Average Score: 18.866  time: 3015.085980653763\n",
      "Episode 67  Episode Score 36.792  Cumulative Average Score: 19.134  time: 3092.9653589725494\n",
      "Episode 68  Episode Score 37.031  Cumulative Average Score: 19.397  time: 3172.3204474449158\n",
      "Episode 69  Episode Score 34.842  Cumulative Average Score: 19.621  time: 3252.351823091507\n",
      "Episode 70  Episode Score 36.810  Cumulative Average Score: 19.866  time: 3333.3101694583893\n",
      "Episode 71  Episode Score 35.807  Cumulative Average Score: 20.091  time: 3415.4831874370575\n",
      "Episode 72  Episode Score 35.980  Cumulative Average Score: 20.312  time: 3498.2538549900055\n",
      "Episode 73  Episode Score 36.857  Cumulative Average Score: 20.538  time: 3582.476941347122\n",
      "Episode 74  Episode Score 36.798  Cumulative Average Score: 20.758  time: 3667.5869040489197\n",
      "Episode 75  Episode Score 36.373  Cumulative Average Score: 20.966  time: 3753.469918012619\n",
      "Episode 76  Episode Score 36.709  Cumulative Average Score: 21.173  time: 3840.5313155651093\n",
      "Episode 77  Episode Score 37.121  Cumulative Average Score: 21.380  time: 3928.2169971466064\n",
      "Episode 78  Episode Score 36.659  Cumulative Average Score: 21.576  time: 4016.636863708496\n",
      "Episode 79  Episode Score 36.795  Cumulative Average Score: 21.769  time: 4106.546157836914\n",
      "Episode 80  Episode Score 35.442  Cumulative Average Score: 21.940  time: 4197.1549162864685\n",
      "Episode 81  Episode Score 36.082  Cumulative Average Score: 22.115  time: 4288.505216598511\n",
      "Episode 82  Episode Score 36.401  Cumulative Average Score: 22.289  time: 4380.813323259354\n",
      "Episode 83  Episode Score 36.024  Cumulative Average Score: 22.454  time: 4474.314990520477\n",
      "Episode 84  Episode Score 36.530  Cumulative Average Score: 22.622  time: 4569.448283433914\n",
      "Episode 85  Episode Score 36.925  Cumulative Average Score: 22.790  time: 4664.429078817368\n",
      "Episode 86  Episode Score 37.874  Cumulative Average Score: 22.965  time: 4760.995890140533\n",
      "Episode 87  Episode Score 37.882  Cumulative Average Score: 23.137  time: 4858.36789727211\n",
      "Episode 88  Episode Score 38.558  Cumulative Average Score: 23.312  time: 4956.3345856666565\n",
      "Episode 89  Episode Score 38.077  Cumulative Average Score: 23.478  time: 5055.670427083969\n",
      "Episode 90  Episode Score 37.596  Cumulative Average Score: 23.635  time: 5155.734085321426\n",
      "Episode 91  Episode Score 38.460  Cumulative Average Score: 23.798  time: 5256.783200263977\n",
      "Episode 92  Episode Score 37.538  Cumulative Average Score: 23.947  time: 5358.910455942154\n",
      "Episode 93  Episode Score 37.400  Cumulative Average Score: 24.092  time: 5462.269307613373\n",
      "Episode 94  Episode Score 38.618  Cumulative Average Score: 24.246  time: 5566.873203754425\n",
      "Episode 95  Episode Score 38.852  Cumulative Average Score: 24.400  time: 5671.811015129089\n",
      "Episode 96  Episode Score 38.166  Cumulative Average Score: 24.544  time: 5777.848053455353\n",
      "Episode 97  Episode Score 36.989  Cumulative Average Score: 24.672  time: 5884.956789493561\n",
      "Episode 98  Episode Score 38.726  Cumulative Average Score: 24.815  time: 5992.406938791275\n",
      "Episode 99  Episode Score 38.428  Cumulative Average Score: 24.953  time: 6101.345470428467\n",
      "Episode 100  Episode Score 38.124  Cumulative Average Score: 25.084  time: 6211.2953362464905\n",
      "Episode 101  Episode Score 38.736  Cumulative Average Score: 25.467  time: 6321.921091794968\n",
      "Episode 102  Episode Score 38.453  Cumulative Average Score: 25.844  time: 6432.947631597519\n",
      "Episode 103  Episode Score 38.403  Cumulative Average Score: 26.222  time: 6544.0765216350555\n",
      "Episode 104  Episode Score 38.158  Cumulative Average Score: 26.595  time: 6654.918837308884\n",
      "Episode 105  Episode Score 38.726  Cumulative Average Score: 26.975  time: 6765.869116783142\n",
      "Episode 106  Episode Score 38.679  Cumulative Average Score: 27.352  time: 6877.0660235881805\n",
      "Episode 107  Episode Score 38.402  Cumulative Average Score: 27.725  time: 6987.853263378143\n",
      "Episode 108  Episode Score 38.257  Cumulative Average Score: 28.095  time: 7098.435721158981\n",
      "Episode 109  Episode Score 38.470  Cumulative Average Score: 28.467  time: 7209.73042845726\n",
      "Episode 110  Episode Score 38.594  Cumulative Average Score: 28.840  time: 7320.154367208481\n",
      "Episode 111  Episode Score 38.408  Cumulative Average Score: 29.207  time: 7431.284583330154\n",
      "Episode 112  Episode Score 39.091  Cumulative Average Score: 29.579  time: 7542.206472873688\n",
      "Episode 113  Episode Score 38.857  Cumulative Average Score: 29.943  time: 7652.642524957657\n",
      "Episode 114  Episode Score 38.649  Cumulative Average Score: 30.303  time: 7763.904206752777\n",
      "\n",
      "Environment solved in 114 episodes!  Cumulative100 Average Score: 30.303\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f28cfa7a1d0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish time: 7764.258405447006\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    with active_session():\n",
    "        # initialize an agent\n",
    "        agent = Agent(state_size = state_size, action_size = action_size, random_seed = 2, num_agents = num_agents)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        scores_all = ddpg()\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot(np.arange(1, len(scores_all)+1), scores_all)\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Episode #')\n",
    "        plt.show()\n",
    "\n",
    "        print('finish time: {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
